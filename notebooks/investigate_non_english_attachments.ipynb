{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate: Non-English Attachments on English pages\n",
    "Notebook explores GOV.UK pages that are non-English attachments but are being marked as English, the default choice.\n",
    "\n",
    "This is part of the Accessibility work to ensure compliance with WCAG. These attachments are currently WCAG fails because screen-reading software that the visually impaired use to read GOV.UK pages will suggest these attachments are English and thus the person will download it, when it the attachment is not actually in English. The consequence is that they will then have to download another attachment, so the page is less accessible.\n",
    "\n",
    "In particular, this notebook lay the code basis so that it can go into the report runner.\n",
    "\n",
    "## Postulation\n",
    "The approach this notebook will take is to identify a column in the pre-processed content store that has the attachment in. We will define this by looking for text that ends in something such as `.pdf`. There are two directions that we can then take:\n",
    "1. Detect language of attachment via the name\n",
    "     + Is easiest method\n",
    "     + Less reliable because names of attachments are typically short and language detection works less effectively when it has less language to scan.\n",
    "1. Detect language of contents of attachment\n",
    "     + Harder as you need to read the attachments in bulk\n",
    "     + All sorts of different attachments such as `.pdf`, `.doc`, `.csv`, `.html` so need a variety of ways to read the contents\n",
    "     + More accurate as will be working with extra text\n",
    "     \n",
    "Option (2.) is on the backburner for now because we would need the full page paths to the attachments and it would be really slow to download all the attachments and read their contents. Can construct it like something below:\n",
    "```\n",
    "str('govuk/' + base_path + `file_name`)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_detectlangs(text):\n",
    "    \"\"\"Detects language of a text, moving onto next text if an error is thrown\n",
    "    \n",
    "    :param text: A string to detect the language of\n",
    "    :return: A list returning the language detected and confidence score associated to it\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return detect_langs(text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_detectlang_df(df):\n",
    "    \"\"\"Apply funcs_detectlangs() function on dataframe columns\n",
    "    \n",
    "    :param df: A dataframe with `text` column to apply func_detectlangs() on\n",
    "    :return: A dataframe with extra column `text_lang` that identifies what language and the level of confidence of the text passed in\n",
    "    \n",
    "    \"\"\"\n",
    "    df['text_lang'] = df['text'].apply(lambda text: func_detectlangs(text))\n",
    "    return df\n",
    "\n",
    "def func_detectlang_pool(df, func, n_cores):\n",
    "    \"\"\"Parallelises the func_detectlang_df function\n",
    "    \n",
    "    :param df: A dataframe to pass into `func`\n",
    "    :param func: A function to apply to dataframe\n",
    "    :param n_cores: Number of cores to parallelise on\n",
    "    \"\"\"\n",
    "    \n",
    "    df_split = np.array_split(df, n_cores)\n",
    "\n",
    "    p = Pool(processes = n_cores)\n",
    "    \n",
    "    df = pd.concat(p.map(func, df_split))\n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of cores on machine\n",
    "n_cores = os.cpu_count() - 1\n",
    "\n",
    "# file attachments\n",
    "file_attachment = \"\"\" \n",
    "    .chm|.csv|.diff|.doc|.docx|.dot|.dxf|.eps|\\\n",
    "    .gif|.gml|.ics|.jpg|.kml|.odp|.ods|.odt|.pdf|\\\n",
    "    .png|.ppt|.pptx|.ps|.rdf|.ris|.rtf|.sch|.txt|\\\n",
    "    .vcf|.wsdl|.xls|.xlsm|.xlsx|.xlt|.xml|.xsd|.xslt|\\ \n",
    "    .zip\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Data used in this will be all the content on GOV.UK that exist on 6th August 2020.\n",
    "\n",
    "Due to the sheer size of the data, need to pre-specify column headings and which columns are dates to make the import process:\n",
    "- Work\n",
    "- Work relatively quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries and headers to specify dtype and date columns\n",
    "dict_header = {'base_path':object,\n",
    "               'content_id':object,\n",
    "               'title':object,\n",
    "               'description':object,\n",
    "               'publishing_app':object,\n",
    "               'document_type':object,\n",
    "               'details':object,  \n",
    "               'text':object,\n",
    "               'organisations':object,  \n",
    "               'taxons':object,\n",
    "               'step_by_steps':object,\n",
    "               'details_parts':object,  \n",
    "               'first_published_at':object,\n",
    "               'public_updated_at':object,\n",
    "               'updated_at':object,\n",
    "               'finder':object,\n",
    "               'facet_values':object,  \n",
    "               'facet_groups':object,\n",
    "               'has_brexit_no_deal_notice':bool,\n",
    "               'withdrawn':bool,\n",
    "               'withdrawn_at':object,\n",
    "               'withdrawn_explanation':object}\n",
    "list_header_date = ['first_published_at',\n",
    "                    'public_updated_at',\n",
    "                    'updated_at',\n",
    "                    'withdrawn_at']\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(filepath_or_buffer='../data/content_store/preprocessed_content_sotre_060820.csv.gz',\n",
    "                 compression='gzip',\n",
    "                 encoding='utf-8',\n",
    "                 sep='\\t',\n",
    "                 header=0,\n",
    "                 names=list(dict_header.keys()),\n",
    "                 dtype=dict_header,\n",
    "                 parse_dates=list_header_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_header, list_header_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(data = {'column': ['this is a .pdf and we also have a .txt', 'this only has .pdf', 'this only has .txt', 'this has nothing']})\n",
    "test['column'].str.contains('.pdf|.txt', na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see that have attachments in `details` column\n",
    "#df.apply(lambda col: col.str.contains('.pdf', na = False), axis = 1)\n",
    "df['details_attachment_exists'] = df['details'].str.contains(file_attachment, na = False)\n",
    "df_attachment = df.query('details_attachment_exists == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachment[['title', 'publishing_app', 'document_type', 'details', 'text', 'public_updated_at']].sample(n = 5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "From previewing the data, spot patterns for extacting `.pdf` files. It looks like they either exist in `src=` or `html=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachment['details'].sample(n = 1000, random_state = 42).to_csv('../data/sample_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Preprocessing\n",
    "Let's extract the file names from the urls so that we can start detecting the language. Will do this in two main stages:\n",
    "1. Extract the urls from the HTML code\n",
    "1. Extract the file names and extensions from the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get smaller cut to work with\n",
    "test = df_attachment[['details']].sample(n = 1000, random_state = 42).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(text):\n",
    "    \"\"\"Extracts the href part of HTML code\n",
    "    \n",
    "    :param text: A string of HTML code that we want to extract the href attribute from\n",
    "    :return: A list of links and other things we have extracted from the href attribute\n",
    "    \n",
    "    \"\"\"\n",
    "    list_links = []\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for a in soup.find_all('a', href = True):\n",
    "        link = a['href']\n",
    "        list_links.append(link)\n",
    "    return list_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['details'].apply(lambda text: BeautifulSoup(text, \"html.parser\").find_all('a', href = True))\n",
    "test['details'] = test['details'].apply(get_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "test['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filename(list_text):\n",
    "    \"\"\"Extracts the last part of a URL path string, including the file name and extension\n",
    "    \n",
    "    :param list_text: A list of strings to extract last part from, e.g. everything after '/'\n",
    "    :return: A list of the same length as list_text, but with the last parts kept e.g. everything after '/'\n",
    "    \n",
    "    \"\"\"\n",
    "    file_name = [os.path.split(text)[1] for text in list_text]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['attachment_name'] = test['details'].apply(extract_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
