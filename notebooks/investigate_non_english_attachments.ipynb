{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate: Non-English Attachments on English pages\n",
    "Notebook explores GOV.UK pages that are non-English attachments but are being marked as English, the default choice.\n",
    "\n",
    "This is part of the Accessibility work to ensure compliance with WCAG. These attachments are currently WCAG fails because screen-reading software that the visually impaired use to read GOV.UK pages will suggest these attachments are English and thus the person will download it, when it the attachment is not actually in English. The consequence is that they will then have to download another attachment, so the page is less accessible.\n",
    "\n",
    "## Approach\n",
    "The approach this notebook will take is to identify a column in the pre-processed content store that has the attachment in. We define this by looking at the *attachment* element of the HTML code and then title relating to this. Generally, there are two directions that we can then take:\n",
    "1. Detect language of attachment via its title\n",
    "     + Is easiest method\n",
    "     + Less reliable because names of attachments are typically short plus there are abbreviations. Language detection works less effectively when it has less language to scan. Just like how humans cannot accurately guess the language of text if they do not have much text to go by.\n",
    "1. Detect language of contents of attachment\n",
    "     + Harder as you need to read the attachments in bulk\n",
    "     + All sorts of different attachments such as `.pdf`, `.doc`, `.csv`, `.html` so need a variety of ways to read the contents\n",
    "     + More accurate as will be working with extra text\n",
    "     \n",
    "We discard Option (2.) because  it would be really slow to download all the attachments and read their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# display multiple outputs in same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attachment_titles(html):\n",
    "    \"\"\"Extracts all the attachment titles from GOV.UK pages\n",
    "    \n",
    "    :param html: String of the HTML code for the GOV.UK page being passed in\n",
    "    :return: list of all the attachment titles that were extracted from GOV.UK page\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # pass html into BeautifulSoup class to apply methods on it\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # initialise list to store results\n",
    "    list_title = []\n",
    "    \n",
    "    # extract all text from `h2` element with class description `title` \n",
    "    # nested in `div` element with class description `attachment-details`\n",
    "    for text in soup.find_all('div', class_ = 'attachment-details'):\n",
    "        for title in text.find_all('h2', class_ = 'title'):\n",
    "            list_title.append(title.get_text())\n",
    "    \n",
    "    return list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_detectlangs(text):\n",
    "    \"\"\"Detects language of a text, moving onto next text if an error is thrown\n",
    "    \n",
    "    :param text: A string to detect the language of\n",
    "    :return: A list returning the language detected and confidence score associated to it\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return [detect_langs(txt) for txt in text]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Data used in this will be all the content on GOV.UK that exist on 6th August 2020.\n",
    "\n",
    "Due to the sheer size of the data, need to pre-specify column headings and which columns are dates to make the import process:\n",
    "- Work\n",
    "- Work relatively quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries and headers to specify dtype and date columns\n",
    "dict_header = {'base_path':object,\n",
    "               'content_id':object,\n",
    "               'title':object,\n",
    "               'description':object,\n",
    "               'publishing_app':object,\n",
    "               'document_type':object,\n",
    "               'details':object,  \n",
    "               'text':object,\n",
    "               'organisations':object,  \n",
    "               'taxons':object,\n",
    "               'step_by_steps':object,\n",
    "               'details_parts':object,  \n",
    "               'first_published_at':object,\n",
    "               'public_updated_at':object,\n",
    "               'updated_at':object,\n",
    "               'finder':object,\n",
    "               'facet_values':object,  \n",
    "               'facet_groups':object,\n",
    "               'has_brexit_no_deal_notice':bool,\n",
    "               'withdrawn':bool,\n",
    "               'withdrawn_at':object,\n",
    "               'withdrawn_explanation':object}\n",
    "list_header_date = ['first_published_at',\n",
    "                    'public_updated_at',\n",
    "                    'updated_at',\n",
    "                    'withdrawn_at']\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(filepath_or_buffer='../data/preprocessed_content_store_200820.csv.gz',\n",
    "                 compression='gzip',\n",
    "                 encoding='utf-8',\n",
    "                 sep='\\t',\n",
    "                 header=0,\n",
    "                 names=list(dict_header.keys()),\n",
    "                 dtype=dict_header,\n",
    "                 parse_dates=list_header_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_header, list_header_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find webpages with attachments on, we assume the following (based on a few case examples):\n",
    "1. They have a non-empty list in the `'attachments': [...]` element\n",
    "\n",
    "Not perfect though, still have pages that don't have any attachments in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have attachments in `details` column, under 'attachments'\n",
    "df['details_attachment_exists'] = df['details'].str.contains('\\'attachments\\'\\: \\[', na = False)\n",
    "df_attachment = df.query('details_attachment_exists == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachment[['title', 'publishing_app', 'document_type', 'details', 'text', 'public_updated_at']].sample(n = 5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Extracting link titles\n",
    "Let's extract the file names from the urls so that we can start detecting the language. Will do this in two main stages:\n",
    "1. Extract the urls from the HTML code\n",
    "1. Extract the file names and extensions from the urls\n",
    "\n",
    "Some example webpages to test are:\n",
    "- [MMR](https://www.gov.uk/government/publications/measles-mumps-and-rubella-lab-confirmed-cases-in-england-2019)\n",
    "- [Dart Charge Bulletin](https://www.gov.uk/government/publications/dart-charge-bulletin-3-advice-for-foreign-hgv-drivers)\n",
    "- [Tribunal decisions](https://www.gov.uk/employment-tribunal-decisions/miss-r-youd-v-elton-community-centre-2404942-2017)\n",
    "    + This example here does not work because need to take `title` from `'attachments'\" [ ... 'title:' ..]` part of HTML code. This seems to be in JSON format and sits outside of the HTML code. Can't quite get it to be treated as a Python dictionary.\n",
    "\n",
    "***\n",
    "\n",
    "```python\n",
    "test = df.query('base_path == \"/employment-tribunal-decisions/miss-r-youd-v-elton-community-centre-2404942-2017\"').iloc[0]['details']\n",
    "soup = BeautifulSoup(test, 'html.parser')\n",
    "print(soup.prettify())\n",
    "\n",
    "test = df.query('base_path == \"/employment-tribunal-decisions/miss-r-youd-v-elton-community-centre-2404942-2017\"').iloc[0]['details']\n",
    "# replace single with double quotes for JSON\n",
    "test = test.replace(\"\\'\", \"\\\"\")\n",
    "test = json.loads(json.dumps(test, separators = \"; \"))\n",
    "\n",
    "isinstance(test, dict)\n",
    "```\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if worked correctly\n",
    "df_extract = df_attachment[['base_path', 'text', 'details']].copy()\n",
    "df_extract['attachment_title'] = df_extract['details'].apply(extract_attachment_titles)\n",
    "\n",
    "df_extract = df_extract[df_extract['attachment_title'].map(lambda x: len(x)) > 0]\n",
    "\n",
    "# for inspection\n",
    "df_extract.sample(n = 1000, random_state = 42).to_csv('../data/sample_attachments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "Let's apply language detection on our attachment titles now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_extract.iloc[:1000,].copy()\n",
    "\n",
    "# get lenght of each attachment list so we can see if language detection works\n",
    "test['attachment_list_length'] = test['attachment_title'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test['attachment_lang'] = test['attachment_title'].apply(lambda x: func_detectlangs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.query('attachment_list_length > 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['base_path'].str.contains('/government/publications/foi-responses-publish')].iloc[0,]['attachment_lang']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs poorly with abbreviations and short sentences, which makes sense."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
