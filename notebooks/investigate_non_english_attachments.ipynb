{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate: Non-English Attachments on English pages\n",
    "Notebook explores GOV.UK pages that are non-English attachments but are being marked as English, the default choice.\n",
    "\n",
    "This is part of the Accessibility work to ensure compliance with WCAG. These attachments are currently WCAG fails because screen-reading software that the visually impaired use to read GOV.UK pages will suggest these attachments are English and thus the person will download it, when it the attachment is not actually in English. The consequence is that they will then have to download another attachment, so the page is less accessible.\n",
    "\n",
    "In particular, this notebook lay the code basis so that it can go into the report runner.\n",
    "\n",
    "## Postulation\n",
    "The approach this notebook will take is to identify a column in the pre-processed content store that has the attachment in. We will define this by looking for text that ends in something such as `.pdf`. There are two directions that we can then take:\n",
    "1. Detect language of attachment via the name\n",
    "     + Is easiest method\n",
    "     + Less reliable because names of attachments are typically short and language detection works less effectively when it has less language to scan.\n",
    "1. Detect language of contents of attachment\n",
    "     + Harder as you need to read the attachments in bulk\n",
    "     + All sorts of different attachments such as `.pdf`, `.doc`, `.csv`, `.html` so need a variety of ways to read the contents\n",
    "     + More accurate as will be working with extra text\n",
    "     \n",
    "Option (2.) is on the backburner for now because we would need the full page paths to the attachments and it would be really slow to download all the attachments and read their contents. Can construct it like something below:\n",
    "```\n",
    "str('govuk/' + base_path + `file_name`)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from langdetect import detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_detectlangs(text):\n",
    "    \"\"\"Detects language of a text, moving onto next text if an error is thrown\n",
    "    \n",
    "    :param text: A string to detect the language of\n",
    "    :return: A list returning the language detected and confidence score associated to it\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return detect_langs(text)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_detectlang_df(df):\n",
    "    \"\"\"Apply funcs_detectlangs() function on dataframe columns\n",
    "    \n",
    "    :param df: A dataframe with `text` column to apply func_detectlangs() on\n",
    "    :return: A dataframe with extra column `text_lang` that identifies what language and the level of confidence of the text passed in\n",
    "    \n",
    "    \"\"\"\n",
    "    df['text_lang'] = df['text'].apply(lambda text: func_detectlangs(text))\n",
    "    return df\n",
    "\n",
    "def func_detectlang_pool(df, func, n_cores):\n",
    "    \"\"\"Parallelises the func_detectlang_df function\n",
    "    \n",
    "    :param df: A dataframe to pass into `func`\n",
    "    :param func: A function to apply to dataframe\n",
    "    :param n_cores: Number of cores to parallelise on\n",
    "    \"\"\"\n",
    "    \n",
    "    df_split = np.array_split(df, n_cores)\n",
    "\n",
    "    p = Pool(processes = n_cores)\n",
    "    \n",
    "    df = pd.concat(p.map(func, df_split))\n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of cores on machine\n",
    "n_cores = os.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Data used in this will be all the content on GOV.UK that exist on 6th August 2020.\n",
    "\n",
    "Due to the sheer size of the data, need to pre-specify column headings and which columns are dates to make the import process:\n",
    "- Work\n",
    "- Work relatively quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries and headers to specify dtype and date columns\n",
    "dict_header = {'base_path':object,\n",
    "               'content_id':object,\n",
    "               'title':object,\n",
    "               'description':object,\n",
    "               'publishing_app':object,\n",
    "               'document_type':object,\n",
    "               'details':object,  \n",
    "               'text':object,\n",
    "               'organisations':object,  \n",
    "               'taxons':object,\n",
    "               'step_by_steps':object,\n",
    "               'details_parts':object,  \n",
    "               'first_published_at':object,\n",
    "               'public_updated_at':object,\n",
    "               'updated_at':object,\n",
    "               'finder':object,\n",
    "               'facet_values':object,  \n",
    "               'facet_groups':object,\n",
    "               'has_brexit_no_deal_notice':bool,\n",
    "               'withdrawn':bool,\n",
    "               'withdrawn_at':object,\n",
    "               'withdrawn_explanation':object}\n",
    "list_header_date = ['first_published_at',\n",
    "                    'public_updated_at',\n",
    "                    'updated_at',\n",
    "                    'withdrawn_at']\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(filepath_or_buffer='../data/preprocessed_content_store_200820.csv.gz',\n",
    "                 compression='gzip',\n",
    "                 encoding='utf-8',\n",
    "                 sep='\\t',\n",
    "                 header=0,\n",
    "                 names=list(dict_header.keys()),\n",
    "                 dtype=dict_header,\n",
    "                 parse_dates=list_header_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_header, list_header_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find webpages with attachments on, we assume the following (based on a few case examples):\n",
    "1. They have a non-empty list in the `'documents': [...]` element\n",
    "1. They have a non-empty list in the `'attachments': [...]` element\n",
    "\n",
    "Not perfect though, still have pages that don't have any attachments in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have attachments in `details` column, under 'attachments'\n",
    "df['details_document_exists'] = df['details'].str.contains('\\'documents\\'\\: \\[', na = False)\n",
    "df['details_attachment_exists'] = df['details'].str.contains('\\'attachments\\'\\: \\[', na = False)\n",
    "df_attachment = df.query('details_document_exists == True & details_attachment_exists == True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachment[['title', 'publishing_app', 'document_type', 'details', 'text', 'public_updated_at']].sample(n = 5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Preprocessing\n",
    "Let's extract the file names from the urls so that we can start detecting the language. Will do this in two main stages:\n",
    "1. Extract the urls from the HTML code\n",
    "1. Extract the file names and extensions from the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case\n",
    "#/government/publications/measles-mumps-and-rubella-lab-confirmed-cases-in-england-2019\n",
    "text_test = df_attachment[df_attachment['base_path'].str.contains(\"/government/publications/dart-charge-bulletin-3-advice-for-foreign-hgv-drivers\")].iloc[0]['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(text_test, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in soup.find_all('div', class_ = 'attachment-details'):\n",
    "    for title in text.find_all('h2', class_ = 'title'):\n",
    "        print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attachment_titles(html):\n",
    "    \"\"\"Extracts all the attachment titles from GOV.UK pages\n",
    "    \n",
    "    :param html: String of the HTML code for the GOV.UK page being passed in\n",
    "    :return: list of all the attachment titles that were extracted from GOV.UK page\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # pass html into BeautifulSoup class to apply methods on it\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # initialise list to store results\n",
    "    list_title = []\n",
    "    \n",
    "    # extract all text from `h2` element with class description `title` \n",
    "    # nested in `div` element with class description `attachment-details`\n",
    "    for text in soup.find_all('div', class_ = 'attachment-details'):\n",
    "        for title in text.find_all('h2', class_ = 'title'):\n",
    "            list_title.append(title.get_text())\n",
    "\n",
    "    \n",
    "    return list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if worked correctly\n",
    "df_extract = df_attachment[['base_path', 'text', 'details']].copy()\n",
    "df_extract['attachment_title'] = df_extract['details'].apply(extract_attachment_titles)\n",
    "df_extract.sample(n = 1000, random_state = 42).to_csv('../data/sample_attachments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
