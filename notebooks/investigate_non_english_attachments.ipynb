{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate: Non-English Attachments on English pages\n",
    "Notebook explores GOV.UK pages that are non-English attachments but are being marked as English, the default choice.\n",
    "\n",
    "This is part of the Accessibility work to ensure compliance with WCAG. These attachments are currently WCAG fails because screen-reading software that the visually impaired use to read GOV.UK pages will suggest these attachments are English and thus the person will download it, when it the attachment is not actually in English. The consequence is that they will then have to download another attachment, so the page is less accessible.\n",
    "\n",
    "## Approach\n",
    "The approach this notebook will take is to identify a column in the pre-processed content store that has the attachment in. We define this by looking at the *attachment* element of the HTML code and then title relating to this. Generally, there are two directions that we can then take:\n",
    "1. Detect language of attachment via its title\n",
    "     + Is easiest method\n",
    "     + Less reliable because names of attachments are typically short plus there are abbreviations. Language detection works less effectively when it has less language to scan. Just like how humans cannot accurately guess the language of text if they do not have much text to go by.\n",
    "1. Detect language of contents of attachment\n",
    "     + Harder as you need to read the attachments in bulk\n",
    "     + All sorts of different attachments such as `.pdf`, `.doc`, `.csv`, `.html` so need a variety of ways to read the contents\n",
    "     + More accurate as will be working with extra text\n",
    "     \n",
    "We discard Option (2.) because  it would be really slow to download all the attachments and read their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import json\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "import multiprocessing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langdetect import detect_langs\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# display multiple outputs in same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_publishing_organisation(content_item, key, index = 0):\n",
    "    \"\"\" Extracts the value of a key within a dictionary masquerading as a string\n",
    "    \n",
    "    :param content_item: A string that's in the format of a dictionary\n",
    "    :param key: The name of the key you want to extract the associated value from\n",
    "    :param index: The index of specific value if you extracted more than one value from the key\n",
    "    :return: the extracted value of the key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # convert object to string\n",
    "        content_item = json.dumps(content_item)\n",
    "        # convert string to object\n",
    "        content_item = json.loads(content_item)\n",
    "\n",
    "        # convert to dictionary\n",
    "        organisations = ast.literal_eval(content_item)\n",
    "\n",
    "        # extract value of key entered from dictionary\n",
    "        organisations = list(map(lambda org: org[index], organisations.get(key, {})))\n",
    "        \n",
    "        return organisations\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [np.NaN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(text):\n",
    "    \"\"\"Extracts all the attachment titles from GOV.UK pages\n",
    "    \n",
    "    :param html: String of the HTML code for the GOV.UK page being passed in\n",
    "    :return: list of all the attachment titles that were extracted from GOV.UK page\n",
    "    \n",
    "    \"\"\"\n",
    "    text = ast.literal_eval(text)\n",
    "    text = text.get('attachments')\n",
    "    \n",
    "    titles = list(map(lambda x: x['title'], text))\n",
    "    \n",
    "    return titles\n",
    "\n",
    "\n",
    "def extract_attachment_titles(html):\n",
    "    \"\"\"Extracts all the attachment titles from GOV.UK pages\n",
    "    \n",
    "    :param html: String of the HTML code for the GOV.UK page being passed in\n",
    "    :return: list of all the attachment titles that were extracted from GOV.UK page\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # pass html into BeautifulSoup class to apply methods on it\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # initialise list to store results\n",
    "    list_title = []\n",
    "    \n",
    "    # extract all text from `h2` element with class description `title` \n",
    "    # nested in `div` element with class description `attachment-details`\n",
    "    for text in soup.find_all('div', class_ = 'attachment-details'):\n",
    "        for title in text.find_all('h2', class_ = 'title'):\n",
    "            list_title.append(title.get_text())\n",
    "    \n",
    "    return list_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_detectlangs(text):\n",
    "    \"\"\"Detects language of a text, moving onto next text if an error is thrown\n",
    "    \n",
    "    :param text: A string to detect the language of\n",
    "    :return: A list returning the language detected and confidence score associated to it\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return detect_langs(text)\n",
    "    except LangDetectException:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Data used in this will be all the content on GOV.UK that exist on 6th August 2020.\n",
    "\n",
    "Due to the sheer size of the data, need to pre-specify column headings and which columns are dates to make the import process:\n",
    "- Work\n",
    "- Work relatively quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries and headers to specify dtype and date columns\n",
    "dict_header = {'base_path':object,\n",
    "               'content_id':object,\n",
    "               'title':object,\n",
    "               'description':object,\n",
    "               'publishing_app':object,\n",
    "               'document_type':object,\n",
    "               'details':object,  \n",
    "               'text':object,\n",
    "               'organisations':object,  \n",
    "               'taxons':object,\n",
    "               'step_by_steps':object,\n",
    "               'details_parts':object,  \n",
    "               'first_published_at':object,\n",
    "               'public_updated_at':object,\n",
    "               'updated_at':object,\n",
    "               'finder':object,\n",
    "               'facet_values':object,  \n",
    "               'facet_groups':object,\n",
    "               'has_brexit_no_deal_notice':bool,\n",
    "               'withdrawn':bool,\n",
    "               'withdrawn_at':object,\n",
    "               'withdrawn_explanation':object}\n",
    "list_header_date = ['first_published_at',\n",
    "                    'public_updated_at',\n",
    "                    'updated_at',\n",
    "                    'withdrawn_at']\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(filepath_or_buffer='../data/preprocessed_content_store_200820.csv.gz',\n",
    "                 compression='gzip',\n",
    "                 encoding='utf-8',\n",
    "                 sep='\\t',\n",
    "                 header=0,\n",
    "                 names=list(dict_header.keys()),\n",
    "                 dtype=dict_header,\n",
    "                 parse_dates=list_header_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data of detected language of content\n",
    "dict_header = {'base_path':object,\n",
    "               'text':object,\n",
    "               'text_languages':object,\n",
    "               'detected_as_english':object}\n",
    "\n",
    "df_lang_detect = pd.read_csv(filepath_or_buffer='../data/non_english_docs_report.csv',\n",
    "                             header=0,\n",
    "                             names=list(dict_header.keys()),\n",
    "                             dtype=dict_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to bring data where we detected the language of the page content, `df_lang_detect` with the data that has the details of the attachment details on them, `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['base_path', 'publishing_app', 'document_type', 'details', 'organisations']].merge(right=df_lang_detect,\n",
    "                                                                                            on='base_path',\n",
    "                                                                                            how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract for non-English pages\n",
    "Here, are manipulating data for non-English pages segment of this work. \n",
    "\n",
    "Further detail in Trello card [here](https://trello.com/c/TkxAtsZD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang_detect = df[['base_path', 'publishing_app', 'document_type', 'organisations']].merge(right=df_lang_detect,\n",
    "                                                                                             on='base_path',\n",
    "                                                                                             how='right')\n",
    "# remove unecessary rows of:\n",
    "# - those that are detected to have English\n",
    "# - those that have NaN in the `text` column\n",
    "df_lang_detect = df_lang_detect.query('detected_as_english == \"False\"')\n",
    "df_lang_detect = df_lang_detect.dropna(subset = ['text'], axis = 'index')\n",
    "\n",
    "# extract `primary_organisation_name`\n",
    "df_lang_detect['primary_publishing_organisation'] = df_lang_detect['organisations'].apply(lambda x: extract_publishing_organisation(content_item = x, \n",
    "                                                                                                                                    key = 'primary_publishing_organisation', \n",
    "                                                                                                                                    index = 1))\n",
    "\n",
    "# select only relevant columns\n",
    "df_lang_detect = df_lang_detect[['base_path', 'primary_publishing_organisation', 'publishing_app', 'document_type', 'text', 'text_languages']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang_detect.to_csv('../data/non_english_page_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_header, list_header_date, df_lang_detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Need to extract organisation titles so analysis can be conducted to spot patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df['organisation_name'] = df['organisations'].apply(lambda x: extract_publishing_organisation(content_item = x, key = 'primary_publishing_organisation', index = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find webpages with attachments on, we assume the following (based on a few case examples):\n",
    "1. They have a non-empty list in the `'attachments': [...]` element\n",
    "\n",
    "Not perfect though, still have pages that don't have any attachments in them. This is probably because `'attachments: []'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have attachments in `details` column, under 'attachments'\n",
    "df['details_attachment_exists'] = df['details'].str.contains('\\'attachments\\'\\: \\[', na = False)\n",
    "df_attachment = df.query('details_attachment_exists == True').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attachment[['base_path', 'organisation_name', 'publishing_app', 'document_type', 'details', 'text', 'text_languages']].sample(n = 5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Extracting link titles\n",
    "Let's extract the file names from the urls so that we can start detecting the language. Will do this in two main stages:\n",
    "1. Extract the urls from the HTML code\n",
    "1. Extract the file names and extensions from the urls\n",
    "\n",
    "Some example webpages to test are:\n",
    "- [MMR](https://www.gov.uk/government/publications/measles-mumps-and-rubella-lab-confirmed-cases-in-england-2019)\n",
    "- [Dart Charge Bulletin](https://www.gov.uk/government/publications/dart-charge-bulletin-3-advice-for-foreign-hgv-drivers)\n",
    "- [Tribunal decisions](https://www.gov.uk/employment-tribunal-decisions/miss-r-youd-v-elton-community-centre-2404942-2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare methods...(from sample extracted as .csv below, the `df['attachment_title_dict']` is more comprehensive than `df['attachment_title_html']`. In particular, there are no missing entries in the former that don't exist in the latter. However, there are missing entries in the latter that don't exist in the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_attachment['attachment_title_dict'] = df_attachment['details'].apply(extract_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_attachment['attachment_title_html'] = df_attachment['details'].apply(extract_attachment_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only pages with actual attachments on\n",
    "df_attachment = df_attachment[df_attachment['attachment_title_dict'].map(lambda d: len(d) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inspection\n",
    "df_attachment.sample(n = 1000, random_state = 42).to_csv('../data/sample_attachments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "Let's apply language detection on our attachment titles now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = multiprocessing.cpu_count() - 1\n",
    "pandarallel.initialize(nb_workers = n_cores, progress_bar=True, use_memory_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create smaller cut of data\n",
    "df_extract = df_attachment[['base_path', 'publishing_app', 'organisation_name', 'document_type', 'text', 'text_languages', 'attachment_title_dict']].copy()\n",
    "# make every list item a row entry\n",
    "df_extract = df_extract.explode('attachment_title_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_extract['attachment_title_lang'] = df_extract['attachment_title_dict'].parallel_apply(func_detectlangs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs poorly with abbreviations and short sentences, which makes sense.\n",
    "\n",
    "Next step is to identify those pages where the language that the content is in, `text_languages`, does not match the language of the attachment title, `attachment_title_lang`.\n",
    "- This is more general compared to those pages that are English but have attachments with a non-English title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_extract = df_extract.rename(columns = {'text_languages': 'text_lang',\n",
    "                                         'attachment_title_dict': 'attachment_title'})\n",
    "\n",
    "# save as different so we don't have to rerun language detection\n",
    "# (note, csv does not retain the nested structures we have for `attachment_title_dict` and `attachment_title_lang`)\n",
    "df_extract.to_csv('../data/df_attachment.csv', index = False)\n",
    "df_extract.to_pickle('../data/df_attachment.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Formatting\n",
    "\n",
    "Now focus on a subset of columns for the data and compare the languages identified in the text with the languages identified from the attachment titles.\n",
    "\n",
    "We will need to do some transformation to:\n",
    "1. for the text and attachment titles, isolate the language code from the confidence scores *e.g. [en: 0.9956]*\n",
    "1. for the attachment titles, *explode* it out so that for each page, each associated row will be one of the attachment titles and the corresponding language\n",
    "1. step 2. allows us to directly compare the predominant language identified for the page text with that related to the attachment title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "df_extract = df_extract.sort_values(by = ['base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extract[['base_path', 'attachment_title', 'attachment_title_lang']].query('base_path == \"/government/publications/foi-responses-published-by-mod-week-commencing-23-november-2015\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[sublists[0] for sublists in lists] for lists in df_extract['attachment_title_lang']][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better way is to convert to list and extract first element but harder\n",
    "df_extract['text_lang_main'] = df_extract['text_languages'].str[1:3]\n",
    "df_extract['attachment_title_lang_main'] = [[sublists[0] for sublists in lists] for lists in df_extract['attachment_title_lang']]\n",
    "\n",
    "# what we want to do next is see if the language in `text_languages` is in this extracted list\n",
    "df_extract[['text_languages', 'text_lang_main', 'attachment_title_lang_main']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first two characters to get language\n",
    "df_extract['attachment_title_lang_main_txt'] = df_extract['attachment_title_lang_main'].astype(str).str[0:2]\n",
    "df_extract[['text_lang_main', 'attachment_title_lang_main_txt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100\n",
    "df_unpivot_title['attachment_title_dict'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.dropna(subset = ['text'])\n",
    "df_output = df_output[['base_path', 'organisation_name', 'publishing_app', 'document_type', 'text_lang', 'attachment_title', 'attachment_title_lang']]\n",
    "df_output.to_csv('../data/non_english_attachment_report.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if text language is same as attachment title language\n",
    "df_extract['check'] = np.where((df_extract['text_lang_main'] == df_extract['attachment_title_lang_main_txt']), True, False)\n",
    "\n",
    "# prepare data output to save as .csv\n",
    "df_output = df_extract.query('check == False').copy()\n",
    "df_output = df_output[['base_path', 'organisation_name', 'publishing_app', 'document_type', 'text', 'text_languages', 'attachment_title_dict', 'attachment_title_lang_main']]\n",
    "df_output = df_output.rename(columns = {'text_languages': 'text_lang',\n",
    "                                        'attachment_title_dict': 'attachment_title',\n",
    "                                        'attachment_title_lang_main': 'attachment_title_lang'})\n",
    "df_output.sort_values(by = ['base_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.sort_values(by = ['base_path']).to_csv('../data/non_english_attachment_report.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
